{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "## Those are the libraries that we will use to create our model and train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "##First, we will import the dataset.\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "#Must convert pandas dataframe to numpy array for matrix algebra.\n",
    "\n",
    "data = data.to_numpy()\n",
    "m,n = data.shape\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 785)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that we have the data, we will split it into the training and testing data.\n",
    "np.random.shuffle(data)\n",
    "train_data = data[:int(0.95*m),:]\n",
    "test_data = data[int(0.95*m):,:]\n",
    "# Using 95% of the data in training and 5% in testing. skewed split towards training since this is a well known dataset.\n",
    "\n",
    "test_data.shape\n",
    "# Gives us 2100 samples to test performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 39900) (39900,)\n",
      "(784, 2100) (2100,)\n",
      "[6 8 8 ... 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "#Now we will split the data into the input and output.\n",
    "X_train = train_data[:,1:].T\n",
    "Y_train = train_data[:,0].T\n",
    "X_test = test_data[:,1:].T\n",
    "Y_test = test_data[:,0].T\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "print(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39900, 10)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#One hot encoding the output.\n",
    "\n",
    "def one_hot(Y):\n",
    "    Y = Y.astype(int)\n",
    "    Y_one_hot = np.zeros((Y.shape[0],10))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y_one_hot[i,Y[i]] = 1\n",
    "    return Y_one_hot.T\n",
    "try_Y = one_hot(Y_train)\n",
    "\n",
    "try_Y = try_Y.T\n",
    "print(try_Y.shape)\n",
    "print(try_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model architecture\n",
    "##  Input layer: 784 neurons (28x28 pixels) (784 inputs)\n",
    "##  Hidden layer: 100 neurons (ReLU activation)\n",
    "##  Output layer: 10 neurons (0-9) (Softmax activation) \n",
    "\n",
    "\n",
    "\n",
    "# Implementation:\n",
    "#  step 1: Initialize weights and biases\n",
    "#  step 2: Forward propagation\n",
    "#  step 3: Backpropagation\n",
    "#  step 4: Update weights and biases\n",
    " \n",
    " \n",
    "def init_params():   # step 1  \n",
    "    W1 = np.random.rand(10,784)\n",
    "    b1 = np.zeros((10,1))\n",
    "    W2 = np.random.rand(10,10)\n",
    "    b2 = np.zeros((10,1))\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def forward_prop(X_train, W1, b1, W2, b2): #step 2\n",
    "    Z1 = np.dot(W1,X_train) + b1\n",
    "    A1 = np.maximum(0,Z1) #ReLU activation: if Z1 is positive, else 0.\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = np.exp(Z2)/np.sum(np.exp(Z2),axis=0,keepdims=True) #Softmax activation\n",
    "    \n",
    "    return Z1,A1, Z2, A2\n",
    "\n",
    "def backprop(X_train, Y_train, Z1,Z2,A1,A2, W2): #step 3\n",
    "    one_hot_y = one_hot(Y_train)\n",
    "    dZ2 = A2 - one_hot_y\n",
    "    dW2 = 1/Y_train.size * np.dot(dZ2,A1.T)\n",
    "    db2 = 1/Y_train.size * np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2) * (A1 > 0) #ReLU derivative: if A1 is positive, else 0.\n",
    "    dW1 = 1/Y_train.size * np.dot(dZ1,X_train.T)\n",
    "    db1 = 1/Y_train.size * np.sum(dZ1,axis=1,keepdims=True)\n",
    "    return dW1,db1,dW2,db2\n",
    "\n",
    "def update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha): #step 4\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1,b1,W2,b2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2837858648.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [156]\u001b[1;36m\u001b[0m\n\u001b[1;33m    return W1,b1,W2,b2\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "  #now implementing gradient descent.\n",
    "def gradient_descent(X_train, Y_train, alpha, num_iter):\n",
    "    W1,b1,W2,b2 = init_params()\n",
    "    for i in range(num_iter):\n",
    "        Z1,A1,Z2,A2 = forward_prop(X_train, W1, b1, W2, b2)\n",
    "        dW1,db1,dW2,db2 = backprop(X_train, Y_train, Z1,Z2,A1,A2, W2)\n",
    "        W1,b1,W2,b2 = update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha)\n",
    "        ## Output logging to help debug and see the training progress.  \n",
    "        if i % 5 == 0:\n",
    "          print(\"Iteration: \",i) # prints iteration number every 10 iterations.\n",
    "          print('prediction: ', np.argmax(A2,0) # prints prediction every 10 iterations.\n",
    "    return W1,b1,W2,b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_14084\\2564532947.py:26: RuntimeWarning: overflow encountered in exp\n",
      "  A2 = np.exp(Z2)/np.sum(np.exp(Z2),axis=0,keepdims=True) #Softmax activation\n",
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_14084\\2564532947.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A2 = np.exp(Z2)/np.sum(np.exp(Z2),axis=0,keepdims=True) #Softmax activation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  5\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  10\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  15\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  20\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  25\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  30\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  35\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  40\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  45\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  50\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  55\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  60\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  65\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  70\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  75\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  80\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Iteration:  85\n",
      "prediction:  [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dhruv\\neuralNetworks-FROM_SCRATCH\\NN_code.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m W1,b1, W2,b2 \u001b[39m=\u001b[39m gradient_descent(X_train, Y_train, \u001b[39m.1\u001b[39;49m, \u001b[39m200\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Dhruv\\neuralNetworks-FROM_SCRATCH\\NN_code.ipynb Cell 8\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(X_train, Y_train, alpha, num_iter)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m W1,b1,W2,b2 \u001b[39m=\u001b[39m init_params()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iter):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     Z1,A1,Z2,A2 \u001b[39m=\u001b[39m forward_prop(X_train, W1, b1, W2, b2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     dW1,db1,dW2,db2 \u001b[39m=\u001b[39m backprop(X_train, Y_train, Z1,Z2,A1,A2, W2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     W1,b1,W2,b2 \u001b[39m=\u001b[39m update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha)\n",
      "\u001b[1;32mc:\\Users\\Dhruv\\neuralNetworks-FROM_SCRATCH\\NN_code.ipynb Cell 8\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(X_train, W1, b1, W2, b2)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop\u001b[39m(X_train, W1, b1, W2, b2): \u001b[39m#step 2\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     Z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(W1,X_train) \u001b[39m+\u001b[39m b1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     A1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m,Z1) \u001b[39m#ReLU activation: if Z1 is positive, else 0.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Dhruv/neuralNetworks-FROM_SCRATCH/NN_code.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     Z2 \u001b[39m=\u001b[39m W2\u001b[39m.\u001b[39mdot(A1) \u001b[39m+\u001b[39m b2\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1,b1, W2,b2 = gradient_descent(X_train, Y_train, .1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93aa56aaf092cc48edb18a400e2b8d907b19181ab850d8edad30d48dcd99f0f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
